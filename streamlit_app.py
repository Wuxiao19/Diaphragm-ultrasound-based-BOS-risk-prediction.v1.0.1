import os
import re
from pathlib import Path
import shutil
import uuid
from random import randint

import numpy as np
import pandas as pd
import streamlit as st

from ultrasound_agent import run_qwen_agent, _build_detection_summary_from_tool_result
import asyncio
import json

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent))
from session_state import get_session_state, persist, load_widget_state


# ============================================================
# Streamlit basic page configuration
# ============================================================

st.set_page_config(
    page_title="Diaphragm Ultrasound Analysis System",
    page_icon="ğŸ©º",
    layout="wide",
)

st.title("Diaphragm Ultrasound Analysis System")
st.markdown(
    """  
Upload **B-mode** and **M-mode** diaphragm ultrasound images
for one patient (single exam) or for multiple patients (batch exams).
The system will automatically perform: feature extraction â†’ feature reduction
â†’ feature fusion â†’ ExtraTrees-based binary classification.

**Filename convention (IMPORTANT):**
- filenames must contain `YY-MM-DD-<ID>` pattern, e.g. `24-05-01-C001_xxx`
"""
)


# ============================================================
# Helper functions: handle uploaded files and temp dirs
# ============================================================

def ensure_upload_dir() -> Path:
    """Ensure base directory for temporary uploaded files exists."""
    base_dir = Path.cwd() / "uploaded_inputs"
    base_dir.mkdir(parents=True, exist_ok=True)
    return base_dir


def _clear_dir(path: Path) -> None:
    """Remove and recreate a directory, ignoring errors if it does not exist."""
    if path.exists():
        shutil.rmtree(path, ignore_errors=True)
    path.mkdir(parents=True, exist_ok=True)


def _new_run_subdir(prefix: str) -> str:
    """
    Create a unique sub-directory name for this run,
    so that different users/runs do not interfere with each other.
    """
    return f"{prefix}_{uuid.uuid4().hex[:12]}"


def save_uploaded_file(uploaded_file, subdir: str) -> str:
    """
    Save a single uploaded file to disk and return its local path.
    """
    upload_root = ensure_upload_dir()
    target_dir = upload_root / subdir
    target_dir.mkdir(parents=True, exist_ok=True)

    file_path = target_dir / uploaded_file.name
    with open(file_path, "wb") as f:
        f.write(uploaded_file.read())
    return str(file_path)


def save_uploaded_files_as_folder(uploaded_files, subdir: str) -> str:
    """
    Save multiple uploaded files to one sub-directory,
    simulating a "folder" input. Return that directory path.
    """
    upload_root = ensure_upload_dir()
    target_dir = upload_root / subdir
    _clear_dir(target_dir)

    for uf in uploaded_files:
        file_path = target_dir / uf.name
        with open(file_path, "wb") as f:
            f.write(uf.read())
    return str(target_dir)


def to_relative_path(abs_path: str) -> str:
    """
    å°†ç»å¯¹è·¯å¾„è½¬æ¢ä¸ºç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„ç›¸å¯¹è·¯å¾„ï¼Œä¸»è¦ç”¨äºæŠŠæœ¬åœ°ä¿å­˜çš„
    uploaded_inputs è·¯å¾„ï¼Œè½¬æ¢æˆ MCP æœåŠ¡å™¨ä¹Ÿèƒ½è¯†åˆ«çš„å½¢å¼ã€‚
    """
    try:
        abs_path_obj = Path(abs_path)
        # ä¼˜å…ˆå°è¯•ï¼šç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•çš„ç›¸å¯¹è·¯å¾„
        try:
            rel_path = abs_path_obj.relative_to(Path.cwd())
            return str(rel_path)
        except ValueError:
            # å¦‚æœæ— æ³•ç›´æ¥ relative_toï¼Œåˆ™å°è¯•æˆªå– "uploaded_inputs" ä¹‹åçš„éƒ¨åˆ†
            parts = abs_path_obj.parts
            if "uploaded_inputs" in parts:
                idx = parts.index("uploaded_inputs")
                rel_parts = parts[idx:]
                return str(Path(*rel_parts))
            # æœ€åå…œåº•ï¼šåªè¿”å›æ–‡ä»¶å
            return abs_path_obj.name
    except Exception:
        # ä»»ä½•å¼‚å¸¸éƒ½ç›´æ¥è¿”å›åŸå§‹è·¯å¾„ï¼Œé¿å…ä¸­æ–­æµç¨‹
        return abs_path


# Keep at most this number of recent detect/runX directories (older ones are removed).
KEEP_LAST_RUNS = 20

# åˆå§‹åŒ– session_state
if "detect_output_dir" not in st.session_state:
    st.session_state["detect_output_dir"] = None


def _clear_session_results():
    """æ¸…é™¤ä¸Šä¸€æ¬¡è¿è¡Œçš„ç»“æœï¼Œé¿å… UI æ˜¾ç¤ºæ··ä¹±ã€‚"""
    st.session_state.pop("agent_result", None)
    st.session_state["detect_output_dir"] = None


def _render_agent_result(ar: dict) -> None:
    """Render agent result stored in session_state or returned from run_qwen_agent.

    This is separated so the UI remains visible across Streamlit reruns (e.g. after
    clicking download_button) because we read from st.session_state.
    """
    if not ar:
        return

    with st.expander("ğŸ”§ æŸ¥çœ‹ï¼šQwen è°ƒç”¨äº†å“ªäº›å·¥å…·", expanded=False):
        if ar.get("tool_calls"):
            for i, tc in enumerate(ar["tool_calls"], 1):
                st.write(f"**å·¥å…· {i}**ï¼š`{tc['name']}`")
                st.json(tc["arguments"])
        else:
            st.write("ï¼ˆæœ¬æ¬¡æœªè°ƒç”¨å·¥å…·ï¼‰")

    with st.expander("ğŸ“Š æŸ¥çœ‹ï¼šå·¥å…·è¿”å›çš„åŸå§‹ JSONï¼ˆè°ƒè¯•ç”¨ï¼‰", expanded=False):
        if ar.get("tool_results"):
            for name, res in ar.get("tool_results", {}).items():
                st.write(f"**{name}** è¿”å›ç»“æœï¼š")
                st.json(res)
        else:
            st.write("ï¼ˆæ— å·¥å…·è¿”å›ç»“æœï¼‰")

    st.markdown("### ğŸ’¬ Qwen Agent çš„å®Œæ•´åˆ†æ")
    detection_summary = None
    if isinstance(ar.get("tool_results"), dict):
        for name, res in ar.get("tool_results", {}).items():
            if isinstance(res, dict) and res:
                try:
                    detection_summary = _build_detection_summary_from_tool_result(res)
                    break
                except Exception:
                    detection_summary = None

    if detection_summary:
        cols = st.columns([1, 1, 1])
        cols[0].metric("æ ·æœ¬æ•°é‡", detection_summary.get("total_samples", 0))
        cols[1].metric("å¹³å‡é£é™©æ¦‚ç‡", f"{detection_summary.get('average_probability', 0.0):.3f}")
        cols[2].metric("å¤æ£€æ‚£è€…æ•°", len(detection_summary.get("recheck_patients", [])))

        st.markdown("**æ ·æœ¬è¯¦æƒ…ï¼ˆè¡¨æ ¼ï¼‰**")
        items_df = pd.DataFrame(detection_summary.get("items", []))
        if not items_df.empty:
            st.dataframe(items_df)

        high_risk = items_df[items_df["risk_probability"] > 0.7] if not items_df.empty else pd.DataFrame()
        if not high_risk.empty:
            st.warning("æ£€æµ‹åˆ°é«˜é£é™©æ‚£è€…ï¼ˆrisk_probability > 0.7ï¼‰ï¼š")
            st.table(high_risk[["merged_key", "patient_id", "date", "risk_probability"]])

        if detection_summary.get("recheck_patients"):
            with st.expander("å¤æ£€æ‚£è€…ï¼ˆåŒä¸€æ‚£è€…åœ¨ä¸åŒæ—¥æœŸçš„éšè®¿ï¼‰", expanded=False):
                for rp in detection_summary.get("recheck_patients", []):
                    st.write(f"æ‚£è€… IDï¼š{rp.get('patient_id')}")
                    st.write("æ£€æŸ¥æ—¥æœŸï¼š" + ", ".join(rp.get("exam_dates", [])))
                    st.dataframe(pd.DataFrame(rp.get("visits", [])))

        if detection_summary.get("missing_modality_summary"):
            ms = detection_summary["missing_modality_summary"]
            st.info(f"ç¼ºå¤±æ¨¡æ€æ ·æœ¬æ•°ï¼š{ms.get('total_missing_samples', 0)}")
            if ms.get("missing_by_type"):
                st.write("æŒ‰ç¼ºå¤±ç±»å‹ç»Ÿè®¡ï¼š")
                st.json(ms.get("missing_by_type"))

    final_text = ar.get("final_response", "")
    if final_text:
        st.markdown("---")
        st.markdown("#### åŸå§‹æ¨¡å‹æ–‡æœ¬è¾“å‡º")
        st.markdown(final_text)
    else:
        st.info("ï¼ˆæ¨¡å‹æœªç”Ÿæˆæ–‡æœ¬ï¼›è¯·æŸ¥çœ‹è°ƒè¯•ä¿¡æ¯ï¼‰")


# ============================================================
# Sidebar: input mode
# ============================================================

st.sidebar.header("Input settings")

input_mode = st.sidebar.radio(
    "Input mode",
    options=["single", "folder"],
    format_func=lambda x: "Single patient (one B + one M image)"
    if x == "single"
    else "Batch patients (multiple B- and M-mode images)",
)


# ============================================================
# Main area: file upload
# ============================================================

st.subheader("1. Upload input data")
st.caption(
    "File naming rule: each filename must start with `YY-MM-DD-<ID>`, "
    "e.g. `24-05-01-C001_xxx.png`. The same patient ID on the same date "
    "will be merged as one exam."
)

col_b, col_m = st.columns(2)

# Ensure dynamic key update for file uploaders
state = get_session_state(b_widget_key=str(randint(1000, 100000000)), m_widget_key=str(randint(1000, 100000000)))

with col_b:
    if input_mode == "single":
        b_file = st.file_uploader(
            "Upload B-mode image (single)",
            type=["jpg", "jpeg", "png", "bmp"],
            key=state.b_widget_key,
        )
    else:
        b_files = st.file_uploader(
            "Upload B-mode images (multiple files allowed, treated as one folder)",
            type=["jpg", "jpeg", "png", "bmp"],
            accept_multiple_files=True,
            key=state.b_widget_key,
        )
    if st.button("Clear B-mode uploads"):
        state.b_widget_key = str(randint(1000, 100000000))
        state.sync()
        st.experimental_rerun()  # Force rerun to refresh uploader

with col_m:
    if input_mode == "single":
        m_file = st.file_uploader(
            "Upload M-mode image (single)",
            type=["jpg", "jpeg", "png", "bmp"],
            key=state.m_widget_key,
        )
    else:
        m_files = st.file_uploader(
            "Upload M-mode images (multiple files allowed, treated as one folder)",
            type=["jpg", "jpeg", "png", "bmp"],
            accept_multiple_files=True,
            key=state.m_widget_key,
        )
    if st.button("Clear M-mode uploads"):
        state.m_widget_key = str(randint(1000, 100000000))
        state.sync()
        st.experimental_rerun()  # Force rerun to refresh uploader

# Ensure variables are defined before use
b_file, b_files, m_file, m_files = None, None, None, None

# Clear previous uploads when new files are uploaded
if "b_image_single" in st.session_state and b_file is not None:
    del st.session_state["b_image_single"]
if "b_image_folder" in st.session_state and b_files:
    del st.session_state["b_image_folder"]
if "m_image_single" in st.session_state and m_file is not None:
    del st.session_state["m_image_single"]
if "m_image_folder" in st.session_state and m_files:
    del st.session_state["m_image_folder"]

# ============================================================
# å…¨å±€ï¼šQwen Agent æ¨¡å¼
# ============================================================
st.markdown("---")
st.subheader("2. AI æ£€æµ‹ä¸è§£è¯»ï¼ˆQwen3-8B Agentï¼‰")
st.caption(
    "è¯´æ˜ï¼šæœ¬æ¨¡å¼ä¸‹ï¼Œä½ åªéœ€è¦åœ¨ä¸Šæ–¹ä¸Šä¼ è†ˆè‚Œ B æ¨¡å¼å’Œ M æ¨¡å¼è¶…å£°å›¾åƒï¼Œ"
    "Qwen ä¼šè‡ªåŠ¨è°ƒç”¨åç«¯çš„æ£€æµ‹æµæ°´çº¿ï¼ˆMCP å·¥å…·ï¼‰ï¼Œå®Œæˆç‰¹å¾æå–å’Œé£é™©é¢„æµ‹ï¼Œç„¶åç»™å‡ºä¸­æ–‡è§£è¯»ã€‚"
    "è¯¥è§£è¯»ä¸èƒ½æ›¿ä»£åŒ»ç”Ÿæœ€ç»ˆè¯Šæ–­ã€‚"
)

with st.expander("ç‚¹å‡»å±•å¼€ï¼šé…ç½® Qwenï¼ˆSiliconFlow/OpenAI å…¼å®¹æ¥å£ï¼‰", expanded=False):
    qwen_api_key = st.text_input(
        "Qwen API Keyï¼ˆå»ºè®®å¡«åˆ°ç¯å¢ƒå˜é‡ QWEN_API_KEYï¼›è¿™é‡Œä¹Ÿå¯ä¸´æ—¶è¾“å…¥ï¼‰",
        type="password",
        value=os.getenv("QWEN_API_KEY", ""),
    )
    qwen_base_url = st.text_input(
        "Base URLï¼ˆä¿æŒé»˜è®¤å³å¯ï¼‰",
        value=os.getenv("QWEN_BASE_URL", "https://api.siliconflow.cn/v1"),
    )
    qwen_model = st.text_input(
        "Modelï¼ˆä¿æŒé»˜è®¤å³å¯ï¼‰",
        value=os.getenv("QWEN_MODEL", "Qwen/Qwen3-8B"),
    )

st.info("ğŸ¤– **Agent æ¨¡å¼**ï¼šç›´æ¥åŸºäºä½ ä¸Šä¼ çš„å›¾åƒï¼Œè°ƒç”¨åç«¯æ£€æµ‹å·¥å…·å¹¶ç”Ÿæˆå®Œæ•´åˆ†æï¼Œæ— éœ€å…ˆç‚¹å‡» Run inferenceã€‚")

if st.button("ğŸš€ å¯åŠ¨ Qwen Agentï¼ˆè‡ªåŠ¨è°ƒç”¨æ£€æµ‹å·¥å…·ï¼‰", type="primary"):
    if not qwen_api_key.strip():
        st.error("è¯·å…ˆå¡«å†™ Qwen API Keyï¼ˆæˆ–åœ¨ç³»ç»Ÿç¯å¢ƒå˜é‡é‡Œè®¾ç½® QWEN_API_KEYï¼‰ã€‚")
    else:
        # æ¸…é™¤ä¸Šä¸€æ¬¡è¿è¡Œçš„ç»“æœ
        _clear_session_results()
        
        # æ ¹æ®å½“å‰è¾“å…¥æ¨¡å¼ï¼Œå‡†å¤‡ä¼ ç»™ Agent çš„æœ¬åœ°è·¯å¾„
        b_path_for_agent = None
        m_path_for_agent = None
        b_folder_for_agent = None
        m_folder_for_agent = None

        try:
            # ä¿é™©ï¼šå†æ¬¡ç¡®ä¿åˆ é™¤ uploaded_inputs ä¸‹çš„æ‰€æœ‰æ—§ä¸Šä¼ å­ç›®å½•
            # ï¼ˆon_change ä¸­åº”è¯¥å·²åˆ é™¤ï¼Œä½†ä¸ºäº†å®‰å…¨å†åšä¸€æ¬¡ï¼‰
            try:
                upload_root = ensure_upload_dir()
                for child in upload_root.iterdir():
                    if child.is_dir():
                        shutil.rmtree(child, ignore_errors=True)
            except Exception:
                pass

            if input_mode == "single":
                if not ("b_file" in locals() and b_file) or not ("m_file" in locals() and m_file):
                    st.error("è¯·å…ˆåœ¨ä¸Šæ–¹ä¸Šä¼ ä¸€å¼  B æ¨¡å¼å’Œä¸€å¼  M æ¨¡å¼å›¾ç‰‡ã€‚")
                    st.stop()

                # ä¿å­˜å•å¼ å›¾ç‰‡åˆ°æœ¬åœ° uploaded_inputs å­ç›®å½•
                b_abs = save_uploaded_file(b_file, _new_run_subdir("B_single_agent"))
                m_abs = save_uploaded_file(m_file, _new_run_subdir("M_single_agent"))

                # ç›´æ¥ä¼ é€’ç»å¯¹è·¯å¾„ï¼Œé¿å… MCP åœ¨ uploaded_inputs æ ¹ç›®å½•ä¸‹æ··åˆæ—§æ–‡ä»¶
                b_path_for_agent = str(Path(b_abs).resolve())
                m_path_for_agent = str(Path(m_abs).resolve())

            else:  # folder æ¨¡å¼
                if not ("b_files" in locals() and b_files) or len(b_files) == 0:
                    st.error("è¯·å…ˆä¸Šä¼ è‡³å°‘ä¸€å¼  B æ¨¡å¼å›¾ç‰‡ï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰ã€‚")
                    st.stop()
                if not ("m_files" in locals() and m_files) or len(m_files) == 0:
                    st.error("è¯·å…ˆä¸Šä¼ è‡³å°‘ä¸€å¼  M æ¨¡å¼å›¾ç‰‡ï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰ã€‚")
                    st.stop()

                b_abs_dir = save_uploaded_files_as_folder(b_files, _new_run_subdir("B_folder_agent"))
                m_abs_dir = save_uploaded_files_as_folder(m_files, _new_run_subdir("M_folder_agent"))

                # ç›´æ¥ä¼ é€’ç»å¯¹ç›®å½•ï¼Œé¿å… MCP æœç´¢åˆ° uploaded_inputs å…¶å®ƒæ—§æ–‡ä»¶
                b_folder_for_agent = str(Path(b_abs_dir).resolve())
                m_folder_for_agent = str(Path(m_abs_dir).resolve())

            # åœ¨å¼€å§‹æ–°ä¸€æ¬¡ Agent è¿è¡Œå‰ï¼Œæ¸…é™¤ä¸Šä¸€æ¬¡çš„æ˜¾ç¤ºï¼ˆä»…åœ¨çœŸæ­£å¼€å§‹è¿è¡Œæ—¶ï¼‰
            st.session_state.pop("agent_result", None)

            with st.spinner("ğŸ¤– Qwen Agent æ­£åœ¨å·¥ä½œï¼šè°ƒç”¨æ£€æµ‹å·¥å…·å¹¶ç”Ÿæˆåˆ†æ..."):
                if b_path_for_agent and m_path_for_agent:
                    agent_result = asyncio.run(
                        run_qwen_agent(
                            b_image_path=b_path_for_agent,
                            m_image_path=m_path_for_agent,
                            api_key=qwen_api_key.strip(),
                            base_url=qwen_base_url.strip(),
                            model=qwen_model.strip(),
                        )
                    )
                elif b_folder_for_agent and m_folder_for_agent:
                    agent_result = asyncio.run(
                        run_qwen_agent(
                            b_folder_path=b_folder_for_agent,
                            m_folder_path=m_folder_for_agent,
                            api_key=qwen_api_key.strip(),
                            base_url=qwen_base_url.strip(),
                            model=qwen_model.strip(),
                        )
                    )
                else:
                    raise ValueError("æ— æ³•ç¡®å®šæ˜¯å•ç»„è¿˜æ˜¯æ‰¹é‡æ¨¡å¼ï¼Œè¯·æ£€æŸ¥ä¸Šä¼ çš„æ–‡ä»¶ã€‚")

            st.success("âœ… Qwen Agent åˆ†æå®Œæˆï¼")

            # å°† agent_result æ¸…ç†ä¸ºçº¯å¯åºåˆ—åŒ–ç»“æ„åä¿å­˜åˆ° session_stateï¼Œ
            # é¿å…åŒ…å«éå¯åºåˆ—åŒ–å¯¹è±¡ï¼ˆå¦‚ Pathã€DataFrame æˆ–è¿æ¥å¥æŸ„ï¼‰å¯¼è‡´
            # Streamlit åœ¨ rerun æ—¶æ— æ³•æŒä¹…åŒ– session_state çš„é—®é¢˜ã€‚
            def _sanitize_agent_result(ar):
                if not isinstance(ar, dict):
                    return ar
                out = {}
                for k, v in ar.items():
                    try:
                        # pandas DataFrame -> records
                        if hasattr(v, "to_dict") and callable(getattr(v, "to_dict")):
                            out[k] = v.to_dict()
                        # numpy types
                        elif isinstance(v, (np.integer, np.floating)):
                            out[k] = v.item()
                        elif isinstance(v, (list, tuple)):
                            new_list = []
                            for e in v:
                                if hasattr(e, "to_dict"):
                                    new_list.append(e.to_dict())
                                else:
                                    new_list.append(e)
                            out[k] = new_list
                        else:
                            out[k] = v
                    except Exception:
                        # å…œåº•ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²è¡¨ç¤º
                        try:
                            out[k] = json.loads(json.dumps(v, default=str))
                        except Exception:
                            out[k] = str(v)
                return out

            sanitized_agent_result = _sanitize_agent_result(agent_result)

            # æŒä¹…åŒ– agent ç»“æœåˆ° session_stateï¼Œè¿™æ ·ä¸‹è½½ç­‰æ“ä½œä¸ä¼šæ¸…é™¤æ˜¾ç¤º
            st.session_state["agent_result"] = sanitized_agent_result
            # æ³¨ï¼šæ¸²æŸ“é€»è¾‘åœ¨é¡µé¢å…¨å±€æœ€åçš„ä»£ç å—ä¸­ï¼Œé¿å…é‡å¤æ¸²æŸ“

            # ====================================================
            # ä» MCP å·¥å…·ç»“æœä¸­è§£æ detect_output_dirï¼ŒåŠ è½½å¹¶å¯¼å‡º CSV
            # ====================================================
            detect_output_dir = None
            ar = st.session_state.get("agent_result")
            for name, res in (ar.get("tool_results", {}) if ar else {}).items():
                if isinstance(res, dict) and "detect_output_dir" in res:
                    detect_output_dir = res["detect_output_dir"]
                    break
            
            # ä¿å­˜åˆ° session_stateï¼Œè¿™æ ·ä¸‹è½½æŒ‰é’®å¯ä»¥åœ¨ä»»ä½• rerun ä¸­è®¿é—®å®ƒ
            if detect_output_dir:
                st.session_state["detect_output_dir"] = detect_output_dir

        except Exception as e:
            st.error(f"âŒ Qwen Agent è¿è¡Œå¤±è´¥ï¼š{e}")
            import traceback
            with st.expander("æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯", expanded=False):
                st.code(traceback.format_exc())

# å¦‚æœ session ä¸­å­˜åœ¨ä¸Šä¸€æ¬¡ agent çš„ç»“æœï¼Œå§‹ç»ˆæ¸²æŸ“å®ƒï¼ˆä¿è¯åœ¨ä»»ä½• rerun åéƒ½å¯è§ï¼‰
if st.session_state.get("agent_result"):
    try:
        _render_agent_result(st.session_state.get("agent_result"))
    except Exception:
        # æ¸²æŸ“å¤±è´¥ä¸åº”é˜»å¡ä¸»æµç¨‹ï¼Œä¿è¯é¡µé¢å…¶å®ƒéƒ¨åˆ†å¯ç”¨
        pass


# ====================================================
# å…¨å±€ï¼šæ˜¾ç¤º CSV ä¸‹è½½å’Œé¢„è§ˆï¼ˆå¦‚æœæœ‰æ£€æµ‹ç»“æœï¼‰
# ====================================================
detect_output_dir = st.session_state.get("detect_output_dir")
if isinstance(detect_output_dir, str) and detect_output_dir:
    result_csv_path = os.path.join(detect_output_dir, "detect_result.csv")
    if os.path.exists(result_csv_path):
        try:
            results_df = pd.read_csv(result_csv_path)

            st.markdown("---")
            st.markdown("### ğŸ“Š æ£€æµ‹ç»“æœé¢„è§ˆï¼ˆæ¥è‡ª MCP æµæ°´çº¿ï¼‰")
            key_cols = [
                "merged_filename",
                "b_filename",
                "m_filename",
                "risk_probability",
                "prediction",
                "prediction_label",
            ]
            show_cols = [c for c in key_cols if c in results_df.columns]
            st.dataframe(results_df[show_cols] if show_cols else results_df)

            st.download_button(
                label="ä¸‹è½½æ£€æµ‹ç»“æœ CSV",
                data=results_df.to_csv(index=False, encoding="utf-8-sig"),
                file_name="detect_result.csv",
                mime="text/csv",
            )

            # ç¼ºå¤±æ¨¡æ€æ ·æœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            missing_csv_path = os.path.join(detect_output_dir, "missing_modality_samples.csv")
            if os.path.exists(missing_csv_path):
                try:
                    missing_df = pd.read_csv(missing_csv_path)
                except Exception:
                    missing_df = None

                if missing_df is not None and not missing_df.empty:
                    st.warning(
                        "éƒ¨åˆ†æ ·æœ¬ç¼ºå¤± B æˆ– M æ¨¡æ€ï¼Œå› æ­¤æœªå‚ä¸æœ€ç»ˆé¢„æµ‹ã€‚"
                        "ä½ å¯ä»¥ä¸‹è½½ç¼ºå¤±æ ·æœ¬åˆ—è¡¨è¿›è¡Œæ’æŸ¥ã€‚"
                    )
                    with st.expander(
                        "Show list of samples with missing modality (downloadable)",
                        expanded=False,
                    ):
                        st.dataframe(missing_df)
                        st.download_button(
                            label="ä¸‹è½½ç¼ºå¤±æ¨¡æ€ CSV",
                            data=missing_df.to_csv(index=False, encoding="utf-8-sig"),
                            file_name="missing_modality_samples.csv",
                            mime="text/csv",
                        )
        except Exception:
            # å¦‚æœè¯»å–å¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹ï¼Œåªæ˜¯ä¸æ˜¾ç¤ºè¡¨æ ¼å’Œä¸‹è½½æŒ‰é’®
            pass

st.markdown("---")
st.caption(
    "Developed by AlMSLab"
)


